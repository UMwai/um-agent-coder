Metadata-Version: 2.4
Name: um-agent-coder
Version: 0.2.0
Summary: Multi-model AI coding agent with parallel execution, task decomposition, and orchestration for long-running tasks
Author: UMwai
License: MIT
Project-URL: Homepage, https://github.com/UMwai/um-agent-coder
Project-URL: Repository, https://github.com/UMwai/um-agent-coder
Project-URL: Issues, https://github.com/UMwai/um-agent-coder/issues
Keywords: ai,agent,llm,coding,orchestration,multi-model,parallel-execution,task-decomposition
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pyyaml>=6.0
Requires-Dist: requests>=2.31.0
Requires-Dist: typing-extensions>=4.8.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21; extra == "dev"
Requires-Dist: black>=23.0; extra == "dev"
Requires-Dist: isort>=5.12; extra == "dev"
Requires-Dist: mypy>=1.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Provides-Extra: openai
Requires-Dist: openai>=1.0.0; extra == "openai"
Provides-Extra: anthropic
Requires-Dist: anthropic>=0.18.0; extra == "anthropic"
Provides-Extra: google
Requires-Dist: google-generativeai>=0.3.0; extra == "google"
Provides-Extra: all
Requires-Dist: um-agent-coder[anthropic,google,openai]; extra == "all"
Dynamic: license-file

# um-agent-coder

A multi-model AI coding agent with parallel execution, task decomposition, and orchestration for **long-running tasks**.

## Features

- **Multi-Model Orchestration**: Route tasks to Gemini (research), Codex (code), and Claude (synthesis)
- **Parallel Execution**: Execute independent subtasks concurrently with dependency tracking
- **Task Decomposition**: Break complex tasks into structured subtasks with model assignments
- **Subagent Spawning**: Spawn isolated subagent processes for true parallelization
- **Checkpointing**: Durable task state for pause/resume capabilities
- **Data Fetchers**: Built-in integrations for SEC EDGAR, Yahoo Finance, ClinicalTrials.gov, News APIs
- **MCP Integration**: Direct MCP tool invocation matching Claude Code patterns (no API keys needed)

## Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/UMwai/um-agent-coder.git
cd um-agent-coder

# Install dependencies
pip install -r requirements.txt
```

### Basic Usage

```bash
# Simple task execution
PYTHONPATH=src python -m um_agent_coder "your task here"

# Multi-model orchestration with parallel execution
PYTHONPATH=src python -m um_agent_coder --orchestrate --parallel "analyze biotech M&A opportunities"

# Subagent process-based execution (true isolation)
PYTHONPATH=src python -m um_agent_coder --orchestrate --parallel --exec-mode subagent "complex task"

# With human approval checkpoints
PYTHONPATH=src python -m um_agent_coder --orchestrate --parallel --human-approval "task requiring review"
```

## Using from Another Repository

If you're in a different repo and want to use um-agent-coder for long-running tasks:

### Option 1: Install as Package

```bash
# From your project directory
pip install git+https://github.com/UMwai/um-agent-coder.git

# Then use in Python
from um_agent_coder.orchestrator import (
    MultiModelOrchestrator,
    TaskDecomposer,
    ParallelExecutor,
    ClaudeCodeSubagentSpawner
)
```

### Option 2: Add as Git Submodule

```bash
# In your project repo
git submodule add https://github.com/UMwai/um-agent-coder.git vendor/um-agent-coder
pip install -r vendor/um-agent-coder/requirements.txt

# Add to PYTHONPATH in your scripts
export PYTHONPATH="${PYTHONPATH}:vendor/um-agent-coder/src"
```

### Option 3: Clone Alongside Your Project

```bash
# Clone next to your project
cd /path/to/your/projects
git clone https://github.com/UMwai/um-agent-coder.git

# In your Python code, add to path
import sys
sys.path.insert(0, '/path/to/your/projects/um-agent-coder/src')
```

## Running Long-Running Tasks

### Example: Complex Research Task

```python
import sys
sys.path.insert(0, '/path/to/um-agent-coder/src')

from um_agent_coder.orchestrator import (
    MultiModelOrchestrator,
    ParallelExecutor,
    ExecutionMode
)
from um_agent_coder.llm.providers.mcp_local import MCPLocalLLM

# Create model instances (uses local MCP tools - NO API keys needed)
gemini = MCPLocalLLM(backend="gemini", model="gemini-3-pro-preview")
codex = MCPLocalLLM(backend="codex", model="o4-mini")
claude = MCPLocalLLM(backend="claude", model="claude-sonnet")

# Create orchestrator
orchestrator = MultiModelOrchestrator(
    gemini=gemini,
    codex=codex,
    claude=claude,
    checkpoint_dir=".my_task_checkpoints",  # For pause/resume
    verbose=True
)

# Run a complex, long-running task
result = orchestrator.run(
    "Analyze biotech companies for M&A opportunities, including pipeline analysis and financial screening"
)

print(result["output"])
```

### Example: Parallel Subagent Execution

```python
from um_agent_coder.orchestrator import (
    ParallelExecutor,
    TaskDecomposer,
    ExecutionMode
)
from um_agent_coder.llm.providers.mcp_local import MCPLocalLLM

# Setup models
gemini = MCPLocalLLM(backend="gemini")
codex = MCPLocalLLM(backend="codex")
claude = MCPLocalLLM(backend="claude")

# Decompose a complex task
decomposer = TaskDecomposer(claude)
decomposed = decomposer.decompose(
    "Build a full-stack feature with API, database, and frontend",
    use_llm=True
)

# Execute in parallel with subagent processes
executor = ParallelExecutor(
    gemini_llm=gemini,
    codex_llm=codex,
    claude_llm=claude,
    execution_mode=ExecutionMode.SUBAGENT_SPAWN,  # Isolated processes
    max_workers=4,
    verbose=True
)

result = executor.execute(decomposed)
```

### Example: Resumable Long-Running Task

```python
from um_agent_coder.agent.enhanced_agent import EnhancedAgent
from um_agent_coder.llm.providers.mcp_local import MCPLocalLLM

# Create agent with checkpointing
agent = EnhancedAgent(
    llm=MCPLocalLLM(backend="claude"),
    checkpoint_dir=".agent_checkpoints"
)

# Start a long task
task_id = agent.execute("Refactor the entire codebase for better modularity")

# Later, if interrupted, resume:
agent.resume(task_id)

# List all tracked tasks
tasks = agent.list_tasks()
for task in tasks:
    print(f"{task['task_id']}: {task['status']}")
```

## CLI Options

| Flag | Description |
|------|-------------|
| `--orchestrate` | Enable multi-model task decomposition |
| `--parallel` | Enable parallel execution of subtasks |
| `--exec-mode` | `sequential`, `threads`, `async`, `subagent` |
| `--human-approval` | Require human approval at checkpoints |
| `--verbose` | Print detailed progress |
| `--decompose-only` | Show task decomposition without executing |

## How It Works

1. **Task Decomposition**: Complex tasks are broken into subtasks with model assignments
   - Gemini: Research, large context analysis, exploration
   - Codex: Code generation, implementation, planning
   - Claude: Synthesis, judgment, final review

2. **Dependency Graph**: Subtasks are organized into parallel execution groups

3. **Parallel Execution**: Independent tasks run concurrently (threads, async, or subagent processes)

4. **Checkpointing**: State is saved after each step for pause/resume

5. **Result Aggregation**: Outputs flow between dependent tasks

## Project Structure

```
src/um_agent_coder/
├── orchestrator/               # Multi-model orchestration
│   ├── task_decomposer.py      # Breaks tasks into subtasks
│   ├── parallel_executor.py    # Parallel execution engine
│   ├── claude_subagent.py      # Subagent process spawning
│   ├── multi_model.py          # Pipeline orchestration
│   └── data_fetchers.py        # SEC, Yahoo Finance, etc.
├── persistence/                # Checkpointing for durability
├── llm/providers/mcp_local.py  # MCP-based LLM provider
└── agent/                      # Core agent with planning

docs/
├── mcp_tool_usage.md           # MCP integration guide
├── claude_subagent_spawning.md # Subagent guide
└── claude_subagent_quick_reference.md

examples/
├── mcp_orchestration_example.py
└── claude_subagent_example.py
```

## LLM Provider Pricing

This section provides a summary of the pricing for the different LLM providers. Please note that this information may not be up-to-date. Always check the official pricing pages for the latest information.

### OpenAI

*   **GPT-4o (2024-08-06):** $2.50 per 1M input tokens, $1.25 for cached input, and $10.00 per 1M output tokens.
*   **GPT-4o Mini:** $0.15 per 1M input tokens, $0.075 for cached input, and $0.60 per 1M output tokens.
*   **GPT-4.5 Preview:** $75.00 per 1M input tokens and $150.00 per 1M output tokens.
*   **GPT-3.5 Turbo:** $0.50 per 1M input tokens and $1.50 per 1M output tokens.

### Anthropic

**Latest Models**

*   **Claude Opus 4.1:**
    *   Input: $15 / MTok
    *   Output: $75 / MTok
*   **Claude Sonnet 4:**
    *   Input: $3 / MTok
    *   Output: $15 / MTok
*   **Claude Haiku 3.5:**
    *   Input: $0.80 / MTok
    *   Output: $4 / MTok

**Legacy Models**

*   **Claude Opus 4:**
    *   Input: $15 / MTok
    *   Output: $75 / MTok
*   **Claude Opus 3:**
    *   Input: $15 / MTok
    *   Output: $75 / MTok
*   **Claude Sonnet 3.7:**
    *   Input: $3 / MTok
    *   Output: $15 / MTok
*   **Claude Haiku 3:**
    *   Input: $0.25 / MTok
    *   Output: $1.25 / MTok

### Google

**Gemini 2.5 Pro**

*   **Input:**
    *   $1.25 per 1M tokens (prompts <= 200k tokens)
    *   $2.50 per 1M tokens (prompts > 200k tokens)
*   **Output:**
    *   $10.00 per 1M tokens (prompts <= 200k tokens)
    *   $15.00 per 1M tokens (prompts > 200k)

**Gemini 2.5 Flash**

*   **Input:**
    *   $0.30 per 1M tokens (text / image / video)
    *   $1.00 per 1M tokens (audio)
*   **Output:** $2.50 per 1M tokens

**Gemini 2.5 Flash-Lite**

*   **Input:**
    *   $0.10 per 1M tokens (text / image / video)
    *   $0.30 per 1M tokens (audio)
*   **Output:** $0.40 per 1M tokens

**Gemini 1.5 Pro**

*   **Input:**
    *   $1.25 per 1M tokens (prompts <= 128k tokens)
    *   $2.50 per 1M tokens (prompts > 128k tokens)
*   **Output:**
    *   $5.00 per 1M tokens (prompts <= 128k tokens)
    *   $10.00 per 1M tokens (prompts > 128k tokens)

**Gemini 1.5 Flash**

*   **Input:**
    *   $0.075 per 1M tokens (prompts <= 128k tokens)
    *   $0.15 per 1M tokens (prompts > 128k tokens)
*   **Output:**
    *   $0.30 per 1M tokens (prompts <= 128k tokens)
    *   $0.60 per 1M tokens (prompts > 128k tokens)
